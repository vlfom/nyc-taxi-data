{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from glob import glob\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import geopandas\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"tlc_raw_data_path\" : \"../raw_data/tlc/\",\n",
    "    \"uber_raw_data_path\" : \"../raw_data/uber/\",\n",
    "    \n",
    "    \"taxi_zones_shapefile\": \"../shapefiles/taxi_zones/taxi_zones.shp\",\n",
    "    \n",
    "    \"parquet_output_path\": \"../processed_data/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_list = {\n",
    "    #     'dropoff_datetime': object, # set by parse_dates in pandas read_csv\n",
    "    'dropoff_latitude': np.float64,\n",
    "    'dropoff_taxizone_id': np.float64,\n",
    "    'dropoff_longitude': np.float64,\n",
    "    'ehail_fee': np.float64,\n",
    "    'extra': np.float64,\n",
    "    'fare_amount': np.float64,\n",
    "    'improvement_surcharge': np.float64,\n",
    "    'junk1': object,\n",
    "    'junk2': object,\n",
    "    'mta_tax': np.float64,\n",
    "    'passenger_count': object,\n",
    "    'payment_type': object,\n",
    "    #     'pickup_datetime': object, # set by parse_dates in pandas read_csv\n",
    "    'pickup_latitude': np.float64,\n",
    "    'pickup_taxizone_id': np.float64,\n",
    "    'pickup_longitude': np.float64,\n",
    "    'rate_code_id': object,\n",
    "    'store_and_fwd_flag': object,\n",
    "    'tip_amount': np.float64,\n",
    "    'tolls_amount': np.float64,\n",
    "    'total_amount': np.float64,\n",
    "    'trip_distance': np.float64,\n",
    "    'trip_type': object,\n",
    "    'vendor_id': object\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Dask client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob(x):\n",
    "    from glob import glob\n",
    "    return sorted(glob(x))\n",
    "\n",
    "def trymakedirs(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_taxi_zones(df, lon_var, lat_var, locid_var):\n",
    "    \"\"\"Joins DataFrame with Taxi Zones shapefile.\n",
    "    This function takes longitude values provided by `lon_var`, and latitude\n",
    "    values provided by `lat_var` in DataFrame `df`, and performs a spatial join\n",
    "    with the NYC taxi_zones shapefile. \n",
    "    The shapefile is hard coded in, as this function makes a hard assumption of\n",
    "    latitude and longitude coordinates. It also assumes latitude=0 and \n",
    "    longitude=0 is not a datapoint that can exist in your dataset. Which is \n",
    "    reasonable for a dataset of New York, but bad for a global dataset.\n",
    "    Only rows where `df.lon_var`, `df.lat_var` are reasonably near New York,\n",
    "    and `df.locid_var` is set to np.nan are updated. \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame or dask.DataFrame\n",
    "        DataFrame containing latitudes, longitudes, and location_id columns.\n",
    "    lon_var : string\n",
    "        Name of column in `df` containing longitude values. Invalid values \n",
    "        should be np.nan.\n",
    "    lat_var : string\n",
    "        Name of column in `df` containing latitude values. Invalid values \n",
    "        should be np.nan\n",
    "    locid_var : string\n",
    "        Name of column in `df` containing taxi_zone location ids. Rows with\n",
    "        valid, nonzero values are not overwritten. \n",
    "    \"\"\"\n",
    "\n",
    "    import geopandas\n",
    "    from shapely.geometry import Point\n",
    "\n",
    "\n",
    "    localdf = df[[lon_var, lat_var, locid_var]].copy()\n",
    "    # localdf = localdf.reset_index()\n",
    "    localdf[lon_var] = localdf[lon_var].fillna(value=0.)\n",
    "    localdf[lat_var] = localdf[lat_var].fillna(value=0.)\n",
    "    localdf['replace_locid'] = (localdf[locid_var].isnull()\n",
    "                                & (localdf[lon_var] != 0.)\n",
    "                                & (localdf[lat_var] != 0.))\n",
    "\n",
    "    if (np.any(localdf['replace_locid'])):\n",
    "        shape_df = geopandas.read_file(config['taxi_zones_shapefile'])\n",
    "        shape_df.drop(['OBJECTID', \"Shape_Area\", \"Shape_Leng\", \"borough\", \"zone\"],\n",
    "                      axis=1, inplace=True)\n",
    "        shape_df = shape_df.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "        try:\n",
    "            local_gdf = geopandas.GeoDataFrame(\n",
    "                localdf, crs={'init': 'epsg:4326'},\n",
    "                geometry=[Point(xy) for xy in\n",
    "                          zip(localdf[lon_var], localdf[lat_var])])\n",
    "\n",
    "            local_gdf = geopandas.sjoin(\n",
    "                local_gdf, shape_df, how='left', op='within')\n",
    "\n",
    "            # one point can intersect more than one zone -- for example if on\n",
    "            # the boundary between two zones. Deduplicate by taking first valid.\n",
    "            local_gdf = local_gdf[~local_gdf.index.duplicated(keep='first')]\n",
    "\n",
    "            local_gdf.LocationID.values[~local_gdf.replace_locid] = (\n",
    "                (local_gdf[locid_var])[~local_gdf.replace_locid]).values\n",
    "\n",
    "            return local_gdf.LocationID.rename(locid_var)\n",
    "        except ValueError as ve:\n",
    "            print(ve)\n",
    "            print(ve.stacktrace())\n",
    "            return df[locid_var].astype(np.float64)\n",
    "    else:\n",
    "        return df[locid_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_green():\n",
    "    green_schema_pre_2015 = \"vendor_id,pickup_datetime,dropoff_datetime,store_and_fwd_flag,rate_code_id,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,total_amount,payment_type,trip_type,junk1,junk2\"\n",
    "    green_glob_pre_2015 = glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'green_tripdata_201[34]*.csv')\n",
    "    )\n",
    "\n",
    "    green_schema_2015_h1 = \"vendor_id,pickup_datetime,dropoff_datetime,store_and_fwd_flag,rate_code_id,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type,junk1,junk2\"\n",
    "    green_glob_2015_h1 = glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2015-0[1-6].csv')\n",
    "    )\n",
    "\n",
    "    green_schema_2015_h2_2016_h1 = \"vendor_id,pickup_datetime,dropoff_datetime,store_and_fwd_flag,rate_code_id,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type\"\n",
    "    green_glob_2015_h2_2016_h1 = \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2015-0[7-9].csv')) + \n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2015-1[0-2].csv')) + \n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2016-0[1-6].csv'))\n",
    "\n",
    "    green_schema_2016_h2 = \"vendor_id,pickup_datetime,dropoff_datetime,store_and_fwd_flag,rate_code_id,pickup_taxizone_id,dropoff_taxizone_id,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type,junk1,junk2\"\n",
    "    green_glob_2016_h2 = \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2016-0[7-9].csv')) +\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2016-1[0-2].csv'))\n",
    "\n",
    "    schemas = [\n",
    "        green_schema_pre_2015, green_schema_2015_h1, green_schema_2015_h2_2016_h1, green_schema_2016_h2\n",
    "    ]\n",
    "    globs = [\n",
    "        green_glob_pre_2015, green_glob_2015_h1, green_glob_2015_h2_2016_h1, green_glob_2016_h2\n",
    "    ]\n",
    "    dates_incides = [\n",
    "        [1, 2], [1, 2], [1, 2], [1, 2]\n",
    "    ]\n",
    "        \n",
    "    critical_columns = [\n",
    "        'pickup_latitude', 'pickup_longitude', 'pickup_datetime', 'pickup_taxizone_id',\n",
    "        'dropoff_latitude', 'dropoff_longitude', 'dropoff_datetime', 'dropoff_taxizone_id',\n",
    "    ]\n",
    "    \n",
    "    green_data_merged = None\n",
    "    for (schema, glob, date_indices) in zip(schemas, globs, dates_incides):\n",
    "        green_df = dd.read_csv(\n",
    "            glob,\n",
    "            parse_dates=date_indices,\n",
    "            names=schema.split(',')\n",
    "            header=0,\n",
    "            na_values=[\"NA\"],\n",
    "            dtype=dtype_list,\n",
    "        )\n",
    "        \n",
    "        green_df = green_df[critical_columns]\n",
    "        \n",
    "        for column in critical_columns:\n",
    "            if column not in green_df.columns:\n",
    "                green_df[column] = np.nan\n",
    "                \n",
    "        green_df = green_df[sorted(green_df.columns)]\n",
    "        \n",
    "        if green_data_merged is None:\n",
    "            green_data_merged = green_df\n",
    "        else:\n",
    "            green_data_merged.append(green_df)\n",
    "\n",
    "    for column in list(green_data_merged.columns):\n",
    "        if column in dtype_list:\n",
    "            green_data_merged[column] = green_data_merged[column].astype(dtype_list[field])\n",
    "\n",
    "    green_data_merged['trip_type'] = 'green'\n",
    "\n",
    "    return green\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tlc_fhv():\n",
    "    fhv_schema_pre_2017 = \"dispatching_base_num,pickup_datetime,pickup_taxizone_id\"\n",
    "    fhv_glob_pre_2017 = glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'fhv_tripdata_201[0-6]*.csv')\n",
    "    )\n",
    "\n",
    "    fhv_schema_2017_h1 = \"dispatching_base_num,dropoff_datetime,pickup_datetime,pickup_taxizone_id,dropoff_taxizone_id\"\n",
    "    fhv_glob_2017_h1 = glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'fhv_tripdata_2017-0[1-6].csv')\n",
    "    )\n",
    "\n",
    "    fhv_schema_2017_h2 = \"dispatching_base_num,dropoff_datetime,pickup_datetime,pickup_taxizone_id,dropoff_taxizone_id,shared_ride_flag\"\n",
    "    fhv_glob_2017_h2 = \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'fhv_tripdata_2017-0[7-9].csv')) +\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'fhv_tripdata_2017-1*.csv'))\n",
    "\n",
    "    fhv_schema_2018 = \"pickup_datetime,dropoff_datetime,pickup_taxizone_id,dropoff_taxizone_id,shared_ride_flag,dispatching_base_num,dispatching_base_num2\"\n",
    "    fhv_glob_2018 = glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'fhv_tripdata_2018*.csv')\n",
    "    )\n",
    "    \n",
    "    schemas = [\n",
    "        fhv_schema_pre_2017, fhv_schema_2017_h1, fhv_schema_2017_h2, fhv_schema_2018\n",
    "    ]\n",
    "    globs = [\n",
    "        fhv_glob_pre_2017, fhv_glob_2017_h1, fhv_glob_2017_h2, fhv_glob_2018\n",
    "    ]\n",
    "    dates_incides = [\n",
    "        [1], [1, 2], [1, 2], [0, 2]\n",
    "    ]\n",
    "    \n",
    "    critical_columns = [\n",
    "        'pickup_latitude', 'pickup_longitude', 'pickup_datetime', 'pickup_taxizone_id',\n",
    "        'dropoff_latitude', 'dropoff_longitude', 'dropoff_datetime', 'dropoff_taxizone_id',\n",
    "    ]\n",
    "    \n",
    "    fhv_data_merged = None\n",
    "    for (schema, glob, date_indices) in zip(schemas, globs, dates_incides):\n",
    "        fhv_df = dd.read_csv(\n",
    "            glob,\n",
    "            parse_dates=date_indices,\n",
    "            names=schema.split(',')\n",
    "            header=0,\n",
    "            na_values=[\"NA\"],\n",
    "            dtype=dtype_list,\n",
    "        )\n",
    "        \n",
    "        fhv_df = fhv_df[critical_columns]\n",
    "        \n",
    "        for column in critical_columns:\n",
    "            if column not in fhv_df.columns:\n",
    "                fhv_df[column] = np.nan\n",
    "                \n",
    "        fhv_df = fhv_df[sorted(fhv_df.columns)]\n",
    "        \n",
    "        if fhv_data_merged is None:\n",
    "            fhv_data_merged = fhv_df\n",
    "        else:\n",
    "            fhv_data_merged.append(fhv_df)\n",
    "\n",
    "    for column in list(fhv_data_merged.columns):\n",
    "        if column in dtype_list:\n",
    "            fhv_data_merged[column] = fhv_data_merged[column].astype(dtype_list[field])\n",
    "\n",
    "    fhv_data_merged['trip_type'] = 'fhv'\n",
    "\n",
    "    return fhv_data_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yellow():\n",
    "    yellow_schema_pre_2015 = \"vendor_id,pickup_datetime,dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,rate_code_id,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,total_amount\"\n",
    "    yellow_glob_pre_2015 = glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_201[0-4]*.csv')) + glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_2009*.csv'))\n",
    "\n",
    "    yellow_schema_2015_2016_h1 = \"vendor_id,pickup_datetime,dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,rate_code_id,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount\"\n",
    "    yellow_glob_2015_2016_h1 = glob(os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_2015*.csv')) + glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_2016-0[1-6].csv'))\n",
    "\n",
    "    yellow_schema_2016_h2 = \"vendor_id,pickup_datetime,dropoff_datetime,passenger_count,trip_distance,rate_code_id,store_and_fwd_flag,pickup_taxizone_id,dropoff_taxizone_id,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount,junk1,junk2\"\n",
    "    yellow_glob_2016_h2 = glob(os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_2016-0[7-9].csv')) + glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_2016-1[0-2].csv'))\n",
    "\n",
    "    yellow1 = dd.read_csv(yellow_glob_pre_2015, header=0,\n",
    "                          na_values=[\"NA\"],\n",
    "                          parse_dates=[1, 2],\n",
    "                          infer_datetime_format=True,\n",
    "                          dtype=dtype_list,\n",
    "                          names=yellow_schema_pre_2015.split(','))\n",
    "    yellow1['dropoff_taxizone_id'] = yellow1['total_amount'].copy()\n",
    "    yellow1['dropoff_taxizone_id'] = np.nan\n",
    "    yellow1['pickup_taxizone_id'] = yellow1['total_amount'].copy()\n",
    "    yellow1['pickup_taxizone_id'] = np.nan\n",
    "    yellow1 = yellow1[[\n",
    "        'dropoff_datetime', 'dropoff_latitude', 'dropoff_longitude', 'dropoff_taxizone_id',\n",
    "        'pickup_datetime', 'pickup_latitude', 'pickup_longitude', 'pickup_taxizone_id',\n",
    "    ]]\n",
    "\n",
    "    yellow2 = dd.read_csv(yellow_glob_2015_2016_h1, header=0,\n",
    "                          na_values=[\"NA\"],\n",
    "                          parse_dates=[1, 2],\n",
    "                          infer_datetime_format=True,\n",
    "                          dtype=dtype_list,\n",
    "                          names=yellow_schema_2015_2016_h1.split(','))\n",
    "    yellow2['dropoff_taxizone_id'] = yellow2['rate_code_id'].copy()\n",
    "    yellow2['dropoff_taxizone_id'] = np.nan\n",
    "    yellow2['pickup_taxizone_id'] = yellow2['rate_code_id'].copy()\n",
    "    yellow2['pickup_taxizone_id'] = np.nan\n",
    "    yellow2 = yellow2[[\n",
    "        'dropoff_datetime', 'dropoff_latitude', 'dropoff_longitude', 'dropoff_taxizone_id',\n",
    "        'pickup_datetime', 'pickup_latitude', 'pickup_longitude', 'pickup_taxizone_id',\n",
    "    ]]\n",
    "\n",
    "    yellow3 = dd.read_csv(yellow_glob_2016_h2, header=0,\n",
    "                          na_values=[\"NA\"],\n",
    "                          parse_dates=[1, 2],\n",
    "                          infer_datetime_format=True,\n",
    "                          dtype=dtype_list,\n",
    "                          names=yellow_schema_2016_h2.split(','))\n",
    "    yellow3['dropoff_latitude'] = yellow3['total_amount'].copy()\n",
    "    yellow3['dropoff_latitude'] = np.nan\n",
    "    yellow3['dropoff_longitude'] = yellow3['total_amount'].copy()\n",
    "    yellow3['dropoff_longitude'] = np.nan\n",
    "    yellow3['pickup_latitude'] = yellow3['total_amount'].copy()\n",
    "    yellow3['pickup_latitude'] = np.nan\n",
    "    yellow3['pickup_longitude'] = yellow3['total_amount'].copy()\n",
    "    yellow3['pickup_longitude'] = np.nan\n",
    "    yellow3 = yellow3[[\n",
    "        'dropoff_datetime', 'dropoff_latitude', 'dropoff_longitude', 'dropoff_taxizone_id',\n",
    "        'pickup_datetime', 'pickup_latitude', 'pickup_longitude', 'pickup_taxizone_id',\n",
    "    ]]\n",
    "\n",
    "    yellow = yellow1[sorted(yellow1.columns)].append(\n",
    "        yellow2[sorted(yellow1.columns)])\n",
    "    yellow = yellow.append(yellow3[sorted(yellow1.columns)])\n",
    "\n",
    "    for field in list(yellow.columns):\n",
    "        if field in dtype_list:\n",
    "            yellow[field] = yellow[field].astype(dtype_list[field])\n",
    "\n",
    "    yellow['trip_type'] = 'yellow'\n",
    "\n",
    "    return yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber():\n",
    "    uber_schema_2014=\"pickup_datetime,pickup_latitude,pickup_longitude,junk1\"\n",
    "    uber_glob_2014 = glob(os.path.join(config['uber_raw_data_path'],'uber*-???14.csv'))\n",
    "\n",
    "    uber_schema_2015=\"junk1,pickup_datetime,junk2,pickup_taxizone_id\"\n",
    "    uber_glob_2015 = glob(os.path.join(config['uber_raw_data_path'],'uber*15.csv'))\n",
    "\n",
    "\n",
    "    uberdf = dd.read_csv(uber_glob_2014, header=0,\n",
    "                         na_values=[\"NA\"], \n",
    "                         parse_dates=[0,],\n",
    "                         infer_datetime_format = True,\n",
    "                         dtype=dtype_list,\n",
    "                         names=uber_schema_2014.split(','))\n",
    "    uberdf = uberdf.drop(['junk1',], axis=1)\n",
    "    uberdf = uberdf.assign(pickup_taxizone_id=np.nan)\n",
    "\n",
    "    default_values = {np.float64: np.nan, np.int64: -999, object: \"\"}\n",
    "\n",
    "    for field in dtype_list:\n",
    "        if (field in uberdf.columns):\n",
    "            uberdf[field] = uberdf[field].astype(dtype_list[field])\n",
    "        elif field == 'pickup_datetime':\n",
    "            pass\n",
    "        else:\n",
    "            uberdf = uberdf.assign(**{field: default_values[dtype_list[field]]})\n",
    "\n",
    "\n",
    "    uberdf = uberdf.drop(['junk1', 'junk2'], axis=1)\n",
    "\n",
    "    uberdf['dropoff_datetime'] = np.datetime64(\"1970-01-01 00:00:00\")\n",
    "    #uberdf = uberdf.repartition(npartitions=20)\n",
    "\n",
    "    uberdf['trip_type'] = 'uber'\n",
    "\n",
    "    uberdf = uberdf[sorted(uberdf.columns)]\n",
    "\n",
    "    return uberdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# uber = get_uber()\n",
    "# green = get_green()\n",
    "yellow = get_yellow()\n",
    "\n",
    "# all_trips = uber.append(green).append(yellow)\n",
    "\n",
    "yellow = yellow[[\n",
    "    'dropoff_datetime', 'dropoff_latitude', 'dropoff_longitude', 'dropoff_taxizone_id',\n",
    "    'pickup_datetime', 'pickup_latitude', 'pickup_longitude', 'pickup_taxizone_id',\n",
    "]]\n",
    "\n",
    "all_trips = yellow\n",
    "\n",
    "all_trips['dropoff_taxizone_id'] = all_trips.map_partitions(\n",
    "    assign_taxi_zones, \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "    \"dropoff_taxizone_id\", meta=('dropoff_taxizone_id', np.float64))\n",
    "all_trips['pickup_taxizone_id'] = all_trips.map_partitions(\n",
    "    assign_taxi_zones, \"pickup_longitude\", \"pickup_latitude\",\n",
    "    \"pickup_taxizone_id\", meta=('pickup_taxizone_id', np.float64))\n",
    "\n",
    "all_trips = all_trips[sorted(all_trips.columns)]\n",
    "\n",
    "all_trips = all_trips.map_partitions(lambda x: x.sort_values('pickup_datetime'), \n",
    "    meta=all_trips)\n",
    "\n",
    "for fieldName in all_trips.columns:\n",
    "    if fieldName in dtype_list:\n",
    "        all_trips[fieldName] = all_trips[fieldName].astype(dtype_list[fieldName])\n",
    "\n",
    "# all_trips.to_parquet(\n",
    "#     os.path.join(config['parquet_output_path'], 'all_trips_unprocessed.parquet'),\n",
    "#     compression='GZIP', has_nulls=True,\n",
    "#     object_encoding='json'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dropoff_datetime', 'dropoff_latitude', 'dropoff_longitude',\n",
       "       'dropoff_taxizone_id', 'pickup_datetime', 'pickup_latitude',\n",
       "       'pickup_longitude', 'pickup_taxizone_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trips.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_trips.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_trips = all_trips.sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = all_trips.shape\n",
    "a = client.submit(sample_trips.compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Future: compute</b> <font color=\"gray\">status: </font><font color=\"black\">pending</font>, <font color=\"gray\">key: </font>compute-d7f38c38075e6f69588b200705001186"
      ],
      "text/plain": [
       "<Future: pending, key: compute-d7f38c38075e6f69588b200705001186>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6min to compute uber2014 size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
