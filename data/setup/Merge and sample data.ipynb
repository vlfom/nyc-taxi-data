{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from glob import glob\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import geopandas\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"tlc_raw_data_path\" : \"../raw_data/tlc/\",\n",
    "    \"uber_raw_data_path\" : \"../raw_data/uber/\",\n",
    "    \n",
    "    \"taxi_zones_shapefile\": \"../shapefiles/taxi_zones/taxi_zones.shp\",\n",
    "    \"nyc_tract_shapefile\": \"../shapefiles/nyct2010_15b/nyct2010.shp\",\n",
    "    \n",
    "    \"parquet_output_path\": \"../processed_data/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_list = {\n",
    "    'DOlocationID': np.float64,\n",
    "    'dropoff_latitude': np.float64,\n",
    "    'dropoff_longitude': np.float64,\n",
    "    'dropoff_taxizone_id': np.float64,\n",
    "    \n",
    "    'locationID': np.float64,\n",
    "    'PUlocationID': np.float64,\n",
    "    'pickup_latitude': np.float64,\n",
    "    'pickup_longitude': np.float64,\n",
    "    'pickup_taxizone_id': np.float64,\n",
    "    \n",
    "    'ehail_fee': np.float64,\n",
    "    'extra': np.float64,\n",
    "    'fare_amount': np.float64,\n",
    "    'improvement_surcharge': np.float64,\n",
    "    'junk1': object,\n",
    "    'junk2': object,\n",
    "    'mta_tax': np.float64,\n",
    "    'passenger_count': object,\n",
    "    'payment_type': object,\n",
    "    'rate_code_id': object,\n",
    "    'store_and_fwd_flag': object,\n",
    "    'tip_amount': np.float64,\n",
    "    'tolls_amount': np.float64,\n",
    "    'total_amount': np.float64,\n",
    "    'trip_distance': np.float64,\n",
    "    'trip_type': object,\n",
    "    'vendor_id': object,\n",
    "    \n",
    "    'pickup_datetime': object,\n",
    "    'dropoff_datetime': object,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Dask client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob(x):\n",
    "    from glob import glob\n",
    "    return sorted(glob(x))\n",
    "\n",
    "def trymakedirs(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_taxi_zones(df, lon_var, lat_var, locid_var):\n",
    "    \"\"\"\n",
    "    Joins DataFrame with Taxi Zones shapefile.\n",
    "    \"\"\"\n",
    "\n",
    "    localdf = df[[lon_var, lat_var, locid_var]].copy()\n",
    "    localdf[lon_var] = localdf[lon_var].fillna(value=0.)\n",
    "    localdf[lat_var] = localdf[lat_var].fillna(value=0.)\n",
    "    localdf['replace_locid'] = (\n",
    "        localdf[locid_var].isnull()\n",
    "        & (localdf[lon_var] != 0.)\n",
    "        & (localdf[lat_var] != 0.)\n",
    "    )\n",
    "\n",
    "    if (np.any(localdf['replace_locid'])):\n",
    "        shape_df = geopandas.read_file(config['taxi_zones_shapefile'])\n",
    "        shape_df.drop(['OBJECTID', \"Shape_Area\", \"Shape_Leng\", \"borough\", \"zone\"],\n",
    "                      axis=1, inplace=True)\n",
    "        shape_df = shape_df.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "        try:\n",
    "            local_gdf = geopandas.GeoDataFrame(\n",
    "                localdf, crs={'init': 'epsg:4326'},\n",
    "                geometry=[Point(xy) for xy in\n",
    "                          zip(localdf[lon_var], localdf[lat_var])])\n",
    "\n",
    "            local_gdf = geopandas.sjoin(\n",
    "                local_gdf, shape_df, how='left', op='within')\n",
    "\n",
    "            local_gdf = local_gdf[~local_gdf.index.duplicated(keep='first')]\n",
    "\n",
    "            local_gdf.LocationID.values[~local_gdf.replace_locid] = (\n",
    "                (local_gdf[locid_var])[~local_gdf.replace_locid]).values\n",
    "\n",
    "            return local_gdf.LocationID.rename(locid_var)\n",
    "        except ValueError as ve:\n",
    "            print(ve)\n",
    "            print(ve.stacktrace())\n",
    "            return df[locid_var].astype(np.float64)\n",
    "    else:\n",
    "        return df[locid_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_nyc_tracts(df, lon_var, lat_var, locid_var):\n",
    "    \"\"\"\n",
    "    Joins DataFrame with NYC Tracts shapefile.\n",
    "    \"\"\"\n",
    "\n",
    "    localdf = df[[lon_var, lat_var, locid_var]].copy()\n",
    "    localdf[lon_var] = localdf[lon_var].fillna(value=0.)\n",
    "    localdf[lat_var] = localdf[lat_var].fillna(value=0.)\n",
    "    localdf['replace_locid'] = (\n",
    "        localdf[locid_var].isnull()\n",
    "        & (localdf[lon_var] != 0.)\n",
    "        & (localdf[lat_var] != 0.)\n",
    "    )\n",
    "\n",
    "    if (np.any(localdf['replace_locid'])):\n",
    "        shape_df = geopandas.read_file(config['nyc_tract_shapefile'])\n",
    "        shape_df = shape_df[[\"BoroCT2010\", \"geometry\"]]\n",
    "        shape_df = shape_df.to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "        try:\n",
    "            local_gdf = geopandas.GeoDataFrame(\n",
    "                localdf, crs={'init': 'epsg:4326'},\n",
    "                geometry=[Point(xy) for xy in\n",
    "                          zip(localdf[lon_var], localdf[lat_var])])\n",
    "\n",
    "            local_gdf = geopandas.sjoin(\n",
    "                local_gdf, shape_df, how='left', op='within')\n",
    "\n",
    "            local_gdf = local_gdf[~local_gdf.index.duplicated(keep='first')]\n",
    "\n",
    "            local_gdf.BoroCT2010.values[~local_gdf.replace_locid] = (\n",
    "                (local_gdf[locid_var])[~local_gdf.replace_locid]).values\n",
    "\n",
    "            return local_gdf.BoroCT2010.rename(locid_var)\n",
    "        except ValueError as ve:\n",
    "            print(ve)\n",
    "            print(ve.stacktrace())\n",
    "            return df[locid_var].astype(np.float64)\n",
    "    else:\n",
    "        return df[locid_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tlc_data(\n",
    "    schemas,\n",
    "    globs,\n",
    "    trip_type\n",
    "):  \n",
    "    critical_columns = [\n",
    "        'pickup_latitude', 'pickup_longitude', 'pickup_datetime', 'pickup_taxizone_id',\n",
    "        'dropoff_latitude', 'dropoff_longitude', 'dropoff_datetime', 'dropoff_taxizone_id',\n",
    "    ]\n",
    "    \n",
    "    df_merged = None\n",
    "    for i in range(len(schemas)):\n",
    "        glob = globs[i]\n",
    "        schema = schemas[i]\n",
    "        \n",
    "        df = dd.read_csv(\n",
    "            glob,\n",
    "            names=schema.split(','),\n",
    "            header=0,\n",
    "            na_values=[\"NA\"],\n",
    "            dtype=dtype_list,\n",
    "        )\n",
    "        \n",
    "        for column in critical_columns:\n",
    "            if column not in df.columns:\n",
    "                df[column] = df[df.columns[0]].copy()\n",
    "                df[column] = np.nan\n",
    "        \n",
    "        df = df[critical_columns]\n",
    "        \n",
    "        for column in critical_columns:\n",
    "            if column in df:\n",
    "                df[column] = df[column].astype(dtype_list[column])\n",
    "        \n",
    "        if df_merged is None:\n",
    "            df_merged = df\n",
    "        else:\n",
    "            df_merged = df_merged.append(df)\n",
    "\n",
    "    for column in list(df_merged.columns):\n",
    "        if column in dtype_list:\n",
    "            df_merged[column] = df_merged[column].astype(dtype_list[column])\n",
    "\n",
    "    df_merged['trip_type'] = trip_type\n",
    "\n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_green():\n",
    "    green_schema_pre_2015 = \"vendor_id,pickup_datetime,dropoff_datetime,store_and_fwd_flag,rate_code_id,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,total_amount,payment_type,trip_type,junk1,junk2\"\n",
    "    green_glob_pre_2015 = glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'green_tripdata_201[34]*.csv')\n",
    "    )\n",
    "\n",
    "    green_schema_2015_h1 = \"vendor_id,pickup_datetime,dropoff_datetime,store_and_fwd_flag,rate_code_id,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type,junk1,junk2\"\n",
    "    green_glob_2015_h1 = glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2015-0[1-6].csv')\n",
    "    )\n",
    "\n",
    "    green_schema_2015_h2_2016_h1 = \"vendor_id,pickup_datetime,dropoff_datetime,store_and_fwd_flag,rate_code_id,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type\"\n",
    "    green_glob_2015_h2_2016_h1 = \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2015-0[7-9].csv')) + \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2015-1[0-2].csv')) + \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2016-0[1-6].csv'))\n",
    "\n",
    "    green_schema_2016_h2 = \"vendor_id,pickup_datetime,dropoff_datetime,store_and_fwd_flag,rate_code_id,pickup_taxizone_id,dropoff_taxizone_id,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type,junk1,junk2\"\n",
    "    green_glob_2016_h2 = \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2016-0[7-9].csv')) + \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2016-1[0-2].csv'))\n",
    "    \n",
    "    green_schema_2017_h1 = \"vendor_id,pickup_datetime,dropoff_datetime,store_and_fwd_flag,rate_code_id,pickup_taxizone_id,dropoff_taxizone_id,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type\"\n",
    "    green_glob_2017_h1 = \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2017-*.csv')) + \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'green_tripdata_2018-*.csv'))\n",
    "\n",
    "    schemas = [\n",
    "        green_schema_pre_2015, green_schema_2015_h1, green_schema_2015_h2_2016_h1, green_schema_2016_h2, green_schema_2017_h1\n",
    "    ]\n",
    "    globs = [\n",
    "        green_glob_pre_2015, green_glob_2015_h1, green_glob_2015_h2_2016_h1, green_glob_2016_h2, green_glob_2017_h1\n",
    "    ]\n",
    "    \n",
    "    return get_tlc_data(\n",
    "        schemas, globs, 'green'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fhv():\n",
    "    fhv_schema_pre_2017 = \"dispatching_base_num,pickup_datetime,pickup_taxizone_id\"\n",
    "    fhv_glob_pre_2017 = glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'fhv_tripdata_201[0-6]*.csv')\n",
    "    )\n",
    "\n",
    "    fhv_schema_2017_h1 = \"dispatching_base_num,pickup_datetime,dropoff_datetime,pickup_taxizone_id,dropoff_taxizone_id\"\n",
    "    fhv_glob_2017_h1 = glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'fhv_tripdata_2017-0[1-6].csv')\n",
    "    )\n",
    "\n",
    "    fhv_schema_2017_h2 = \"dispatching_base_num,pickup_datetime,dropoff_datetime,pickup_taxizone_id,dropoff_taxizone_id,shared_ride_flag\"\n",
    "    fhv_glob_2017_h2 = \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'fhv_tripdata_2017-0[7-9].csv')) + \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'fhv_tripdata_2017-1*.csv'))\n",
    "\n",
    "    fhv_schema_2018 = \"pickup_datetime,dropoff_datetime,pickup_taxizone_id,dropoff_taxizone_id,shared_ride_flag,dispatching_base_num,dispatching_base_num2\"\n",
    "    fhv_glob_2018 = glob(\n",
    "        os.path.join(config['tlc_raw_data_path'], 'fhv_tripdata_2018*.csv')\n",
    "    )\n",
    "    \n",
    "    schemas = [\n",
    "        fhv_schema_pre_2017, fhv_schema_2017_h1, fhv_schema_2017_h2, fhv_schema_2018\n",
    "    ]\n",
    "    globs = [\n",
    "        fhv_glob_pre_2017, fhv_glob_2017_h1, fhv_glob_2017_h2, fhv_glob_2018\n",
    "    ]\n",
    "        \n",
    "    return get_tlc_data(\n",
    "        schemas, globs, 'fhv'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yellow():\n",
    "    yellow_schema_pre_2015 = \"vendor_id,pickup_datetime,dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,rate_code_id,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,total_amount\"\n",
    "    yellow_glob_pre_2015 = \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_201[0-4]*.csv')) + \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_2009*.csv'))\n",
    "\n",
    "    yellow_schema_2015_2016_h1 = \"vendor_id,pickup_datetime,dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,rate_code_id,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount\"\n",
    "    yellow_glob_2015_2016_h1 = \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_2015*.csv')) + \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_2016-0[1-6].csv'))\n",
    "\n",
    "    yellow_schema_2016_h2 = \"vendor_id,pickup_datetime,dropoff_datetime,passenger_count,trip_distance,rate_code_id,store_and_fwd_flag,pickup_taxizone_id,dropoff_taxizone_id,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount,junk1,junk2\"\n",
    "    yellow_glob_2016_h2 = \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_2016-0[7-9].csv')) + \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_2016-1[0-2].csv'))\n",
    "    \n",
    "    yellow_schema_2017_h1=\"vendor_id,pickup_datetime,dropoff_datetime,passenger_count,trip_distance,rate_code_id,store_and_fwd_flag,pickup_taxizone_id,dropoff_taxizone_id,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount\"\n",
    "    yellow_glob_2017_h1 = \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_2017-*.csv')) + \\\n",
    "        glob(os.path.join(config['tlc_raw_data_path'], 'yellow_tripdata_2018-*.csv'))\n",
    "    \n",
    "    schemas = [\n",
    "         yellow_schema_pre_2015, yellow_schema_2015_2016_h1, yellow_schema_2016_h2, yellow_schema_2017_h1\n",
    "    ]\n",
    "    globs = [\n",
    "         yellow_glob_pre_2015, yellow_glob_2015_2016_h1, yellow_glob_2016_h2, yellow_glob_2017_h1\n",
    "    ]\n",
    "        \n",
    "    return get_tlc_data(\n",
    "        schemas, globs, 'yellow'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber():\n",
    "    uber_schema_2014 = \"pickup_datetime,pickup_latitude,pickup_longitude,junk1\"\n",
    "    uber_glob_2014 = glob(os.path.join(config['uber_raw_data_path'],'uber*-???14.csv'))\n",
    "    \n",
    "    uber_schema_2015 = \"junk1,pickup_datetime,junk2,pickup_taxizone_id\"\n",
    "    uber_glob_2015 = glob(os.path.join(config['uber_raw_data_path'],'uber*15.csv'))\n",
    "    \n",
    "    schemas = [\n",
    "         uber_schema_2014, uber_schema_2015\n",
    "    ]\n",
    "    globs = [\n",
    "         uber_glob_2014, uber_glob_2015\n",
    "    ]\n",
    "        \n",
    "    return get_tlc_data(\n",
    "        schemas, globs, 'uber'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all\n",
    "\n",
    "uber = get_uber()\n",
    "fhv = get_fhv()\n",
    "green = get_green()\n",
    "yellow = get_yellow()\n",
    "\n",
    "all_trips = uber.append(fhv).append(green).append(yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(df, frac=0.02):\n",
    "    from scipy import stats\n",
    "    return stats.bernoulli.rvs(frac, size=df.shape[0])\n",
    "\n",
    "all_trips[\"sample\"] = all_trips.map_partitions(\n",
    "    sample, meta=('sample', np.float64)\n",
    ")\n",
    "all_trips = all_trips[all_trips[\"sample\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map coordinates to taxi zones\n",
    "\n",
    "all_trips['pickup_taxizone_id'] = all_trips.map_partitions(\n",
    "    assign_taxi_zones,\n",
    "    \"pickup_longitude\", \"pickup_latitude\", \"pickup_taxizone_id\",\n",
    "    meta=('pickup_taxizone_id', np.float64)\n",
    ")\n",
    "all_trips['dropoff_taxizone_id'] = all_trips.map_partitions(\n",
    "    assign_taxi_zones,\n",
    "    \"dropoff_longitude\", \"dropoff_latitude\", \"dropoff_taxizone_id\",\n",
    "    meta=('dropoff_taxizone_id', np.float64)\n",
    ")\n",
    "\n",
    "all_trips['dropoff_taxizone_tract_id'] = all_trips['pickup_taxizone_id'].copy()\n",
    "all_trips['dropoff_taxizone_tract_id'] = np.nan\n",
    "all_trips['dropoff_taxizone_tract_id'] = all_trips.map_partitions(\n",
    "    assign_nyc_tracts,\n",
    "    \"dropoff_longitude\", \"dropoff_latitude\", \"dropoff_taxizone_tract_id\",\n",
    "    meta=('dropoff_taxizone_tract_id', np.float64)\n",
    ")\n",
    "\n",
    "all_trips['pickup_taxizone_tract_id'] = all_trips['pickup_taxizone_id'].copy()\n",
    "all_trips['pickup_taxizone_tract_id'] = np.nan\n",
    "all_trips['pickup_taxizone_tract_id'] = all_trips.map_partitions(\n",
    "    assign_nyc_tracts,\n",
    "    \"pickup_longitude\", \"pickup_latitude\", \"pickup_taxizone_tract_id\",\n",
    "    meta=('pickup_taxizone_tract_id', np.float64)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trips = all_trips[[\n",
    "    'pickup_datetime',\n",
    "    'pickup_taxizone_id',\n",
    "    'dropoff_datetime',\n",
    "    'dropoff_taxizone_id',\n",
    "    'trip_type'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save to .parquet\n",
    "\n",
    "all_trips.to_parquet(\n",
    "    os.path.join(config['parquet_output_path'], '2perc_sample.parquet'),\n",
    "    compression='GZIP',\n",
    "    has_nulls=True,\n",
    "    object_encoding='json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final size: ~500MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
